[{"id":0,"href":"/documentation/Diagrams/Architecture-Diagram/","title":"Architecture Diagram","parent":"Diagrams","content":"The data relative to the number of people detected by Wi-Fi sniffing and object detection and the number of moliceiros detected by object detection is captured by the respective modules in the sensors and this data is sent to the broker through a MQTT connection. The broker saves the data in a database and also sends it to the Web service (Flask). The web server (Gunicorn) interacts with the Web service and communicates with the Reverse Proxy Server (Nginx) through a Unix socket. The Reverse Proxy Server manages the load balance of requests. The user visualizes the web application in his device, sending and receiving HTTPS requests and responses.\n "},{"id":1,"href":"/documentation/Diagrams/Deployment-Diagram/","title":"Deployment Diagram","parent":"Diagrams","content":"In this diagram it is possible to view the several components of the system and how they interact between them.\n "},{"id":2,"href":"/documentation/Diagrams/Domain-Diagram/","title":"Domain Diagram","parent":"Diagrams","content":"The domain model of the project represents the real-world conceptual classes of the system, their attributes and how they are associated.\n "},{"id":3,"href":"/documentation/Diagrams/Use-Cases-Diagrams/","title":"Use Cases Diagrams","parent":"Diagrams","content":"API     Web Application     "},{"id":4,"href":"/documentation/Modules/Object-Detection/","title":"Object Detection","parent":"Modules","content":"Context    This module allows the detection of three diferent types of objects (people, vehicles and two wheeler vehicles) using a SDK called DeepStream running on a service in a Jetson Nano.\nMain functions    The main functions of the program are:\n tiler_src_pad_buffer_probe(pad,info,u_data) - tiler_sink_pad_buffer_probe will extract metadata received on OSD sink pad and update params for drawing rectangle, object information etc. create_source_bin(index,uri) - Create a source GstBin to abstract this bin\u0026rsquo;s content from the rest of the pipeline cb_newpad(decodebin, decoder_src_pad,data) - Gets the source bin ghost pad and checks if the pad created by the decodebin is for video and not audio main() - Creates a Pipeline element that will form a connection of other elements, a nvstreammux instance to form batches from one or more sources, a tiler and an egl sink  Detection    def tiler_src_pad_buffer_probe(pad,info,u_data): frame_number=0 num_rects=0 is_first_object=True gst_buffer = info.get_buffer() if not gst_buffer: print(\u0026#34;Unable to get GstBuffer \u0026#34;) return # Retrieve batch metadata from the gst_buffer # Note that pyds.gst_buffer_get_nvds_batch_meta() expects the # C address of gst_buffer as input, which is obtained with hash(gst_buffer) batch_meta = pyds.gst_buffer_get_nvds_batch_meta(hash(gst_buffer)) l_frame = batch_meta.frame_meta_list while l_frame is not None: try: # Note that l_frame.data needs a cast to pyds.NvDsFrameMeta # The casting is done by pyds.NvDsFrameMeta.cast() # The casting also keeps ownership of the underlying memory # in the C code, so the Python garbage collector will leave # it alone. frame_meta = pyds.NvDsFrameMeta.cast(l_frame.data) except StopIteration: continue is_first_object = True frame_number=frame_meta.frame_num l_obj=frame_meta.obj_meta_list num_rects = frame_meta.num_obj_meta obj_counter = { PGIE_CLASS_ID_VEHICLE:0, PGIE_CLASS_ID_PERSON:0, PGIE_CLASS_ID_BICYCLE:0, PGIE_CLASS_ID_ROADSIGN:0 } while l_obj is not None: try: # Casting l_obj.data to pyds.NvDsObjectMeta obj_meta=pyds.NvDsObjectMeta.cast(l_obj.data) except StopIteration: continue obj_counter[obj_meta.class_id] += 1 # Ideally NVDS_EVENT_MSG_META should be attached to buffer by the # component implementing detection / recognition logic. # Here it demonstrates how to use / attach that meta data. if(is_first_object): # Allocating an NvDsEventMsgMeta instance and getting reference # to it. The underlying memory is not manged by Python so that # downstream plugins can access it. Otherwise the garbage collector # will free it when this probe exits. msg_meta=pyds.alloc_nvds_event_msg_meta() msg_meta.bbox.top = obj_meta.rect_params.top #Holds top coordinate of the box in pixels msg_meta.bbox.left = obj_meta.rect_params.left #Holds left coordinate of the box in pixels msg_meta.bbox.width = obj_meta.rect_params.width #Holds width of the box in pixels. msg_meta.bbox.height = obj_meta.rect_params.height #Holds height of the box in pixels if msg_meta.bbox.left \u0026lt; 300: #retira area da Rua da Pega obj_counter[obj_meta.class_id] -= 1 is_first_object = True try: l_obj=l_obj.next except StopIteration: break global num_people global num_vehicles global num_twowheelers num_people = obj_counter[PGIE_CLASS_ID_PERSON] num_vehicles = obj_counter[PGIE_CLASS_ID_VEHICLE] num_twowheelers = obj_counter[PGIE_CLASS_ID_BICYCLE] # Get frame rate through this probe fps_streams[\u0026#34;stream{0}\u0026#34;.format(frame_meta.pad_index)].get_fps() try: l_frame=l_frame.next except StopIteration: break return Gst.PadProbeReturn.OK Local Broker    There are 2 types of values: unique values and current values.\nThe current values are captured every second after analysing 12.4 video frames. This means that every second a message with the number of detected people, vehicles and two wheeler vehicles is sent to the local broker.\nThe unique values are sent in a time interval of 60 seconds and they are the sum of every new object that appeared during that time interval in each category. That means that if during a minute 22 new people apear on the video feed, the people unique value at the end of that minute will be 22.\nThis captured data is sent to a local broker in 6 different topics:\n#current currentppl_topic = \u0026#34;detection/people/current\u0026#34; currentvehicles_topic = \u0026#34;detection/vehicle/current\u0026#34; currenttwowheelers_topic = \u0026#34;detection/twowheelers/current\u0026#34; #unique uniqueppl_topic = \u0026#34;detection/people/unique\u0026#34; uniquevehicles_topic = \u0026#34;detection/vehicle/unique\u0026#34; uniquetwowheelers_topic = \u0026#34;detection/twowheelers/unique\u0026#34; Since the data has to be sent in different time intervals, its handling has to be separate. To prevent sending several messages with the value 0 for hours (like for example during the night) to the broker, the code was changed in order to supress sending repeated zeros. To do this, every time a message is sent, it has to be checked if the previous message was a 0 and if the current message is also a 0.\ncount = 0 while True: if count == 60: previous_unique_p = num_people_unique previous_unique_v = num_vehicles_unique previous_unique_t = num_twowheelers_unique if not previous_unique_p == 0 or num_people_unique != 0: unpeople = json.dumps({\u0026#34;value\u0026#34; : num_people_unique, \u0026#34;TimeStamp\u0026#34; : time.time()}) client.publish(uniqueppl_topic, unpeople) if not previous_unique_v == 0 or num_vehicles_unique != 0: unvehicles = json.dumps({\u0026#34;value\u0026#34; : num_vehicles_unique, \u0026#34;TimeStamp\u0026#34; : time.time()}) client.publish(uniquevehicles_topic, unvehicles) if not previous_unique_t == 0 or num_twowheelers_unique != 0: untwowheelers = json.dumps({\u0026#34;value\u0026#34; : num_twowheelers_unique, \u0026#34;TimeStamp\u0026#34; : time.time()}) client.publish(uniquetwowheelers_topic, untwowheelers) num_people_unique = 0 num_vehicles_unique = 0 num_twowheelers_unique = 0 count = 0 time.sleep(1) #manda os valores de 1 em 1 segundo dif = (num_people - bk_numbers[0], num_vehicles - bk_numbers[1], num_twowheelers - bk_numbers[2]) if dif[0] \u0026gt; 0: num_people_unique += dif[0] if dif[1] \u0026gt; 0: num_vehicles_unique += dif[1] if dif[2] \u0026gt; 0: num_twowheelers_unique += dif[2] count += 1 if not previous_current_p == 0 or num_people != 0: crpeople = json.dumps({\u0026#34;value\u0026#34; : num_people, \u0026#34;TimeStamp\u0026#34; : time.time()}) client.publish(currentppl_topic, crpeople) previous_current_p = num_people if not previous_current_v == 0 or num_vehicles != 0: crvehicles = json.dumps({\u0026#34;value\u0026#34; : num_vehicles, \u0026#34;TimeStamp\u0026#34; : time.time()}) client.publish(currentvehicles_topic, crvehicles) previous_current_v = num_vehicles if not previous_current_t == 0 or num_twowheelers != 0: crtwowheelers = json.dumps({\u0026#34;value\u0026#34; : num_twowheelers, \u0026#34;TimeStamp\u0026#34; : time.time()}) client.publish(currenttwowheelers_topic, crtwowheelers) previous_current_t = num_twowheelers bk_numbers = (num_people, num_vehicles, num_twowheelers) Connection to Central Broker    All of the messages sent in topics to the local brokers are then sent to the central broker in IT to be accessed in the web application.\nRunning the program    To run the program in the Jetson Nano:\npython3 detector2.py rtsp://admin:admin@192.168.115.9/11 To continue running after closing the ssh session:\n nohup python3 -u detector2.py rtsp://admin:admin@192.168.115.9/11 \u0026lt;/dev/null \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp; References    Guide on how to install DeepStream SDK on the Jetson Nano\n"},{"id":5,"href":"/documentation/Modules/Server-Configuration/","title":"Server Configuration","parent":"Modules","content":"The following topics describe the server configuration following the work flow to do so:\n The WebAppDev repository was cloned to the virtual machine  Gunicorn     A service of gunicorn was created to run the web application using gevent on the same address and port of socket instanced on the web application. The file is located in /etc/systemd/system/webapp.service  [Unit] Description=Gunicorn instance to serve PEI2021_NAP project After=network.target [Service] User=root Group=www-data WorkingDirectory=/atcll-webapp-flask Environment=\u0026quot;PATH=/atcll-webapp-flask/venv/bin\u0026quot; ExecStart=/atcll-webapp-flask/venv/bin/gunicorn '__init__:create_app()' -k gevent --worker-connections 1000 --bind 0.0.0.0:5000 [Install] WantedBy=multi-user.target It\u0026rsquo;s important to mention the gunicorn run atributes (gunicorn \u0026lsquo;init:create_app()\u0026rsquo; -k gevent \u0026ndash;worker-connections 1000 \u0026ndash;bind 0.0.0.0:5000)\n gevent \u0026ndash;worker-connections 1000 These attributes allow for the server to handle multiple requests and use server sent envents \u0026ndash;bind=\u0026ldquo;0.0.0.0:5000\u0026rdquo; Indicates gunicorn the address and port the app and socket is running, NEEDS TO BE THE SAME AS THE ONE IN THE NGINX CONFIGURATION FILE 'init:create_app()' Indicates gunicorn the name and attribute of the flask web application  We need to run the webapp service, so it will always be running using sudo systemctl start webapp\nNGINX    We then need to configure the NGINX reverse proxy server, we need to link the local gunicorn and redirect it to the dev.aveiro-open-lab.pt.\n The file is located in /etc/nginx/sites-available/webapp  We start the file by pointing the / in the server to the local web app (0:0:0:0:5000)\nserver{ server_name dev.aveiro-open-lab.pt; location / { include proxy_params; proxy_pass http://0.0.0.0:5000; proxy_buffering off; proxy_cache off; proxy_set_header Host $host; proxy_set_header Connection ''; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_http_version 1.1; chunked_transfer_encoding off; } The rest of the file was generated by certbot so we can have the webapp running on https\n listen 443 ssl; # managed by Certbot ssl_certificate /etc/letsencrypt/live/dev.aveiro-open-lab.pt/fullchain.pem; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/dev.aveiro-open-lab.pt/privkey.pem; # managed by Certbot include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot } server{ if ($host = dev.aveiro-open-lab.pt) { return 301 https://$host$request_uri; } # managed by Certbot listen 80; server_name dev.aveiro-open-lab.pt; return 404; # managed by Certbot } We need to run the nginx service, so it will always be running using sudo systemctl start nginx\n"},{"id":6,"href":"/documentation/Modules/Sniffing/","title":"Sniffing","parent":"Modules","content":"Context    The sniffing module is written in Python3 and runs as a service on the Smart Lamp Post APU. It is responsible for capturing probe requests packets and send to a broker values about current devices as well as unique devices that have passed by the post, in real-time.\nDescription    The module uses PyShark which is a wrapper for TShark (TShark is a terminal based version of wireshark). In order to capture WiFi packets, we turned on monitor mode on wlan1 interface of the APU\u0026rsquo;s wireless network adapter. The PyShark\u0026rsquo;s capture filter is set to probe requests, which are packets that are sent by a device scanning for access points in the area and are sent periodically. From these packets we are able to extract each device\u0026rsquo;s mac address and use it for our statistics purpose.\nHow to configure the lamp post APU wlan1 interface into monitor mode    ifconfig wlan1 down iwconfig wlan1 mode monitor ifconfig wlan1 up Threads    In this file there are 2 running threads and 1 main function running permanently: chopping() thread, run_broker() thread and the main function where the PyShark is capturing the packets. The first thread, chopping, is responsible for constantly switching wifi channels through 1, 6 and 11. The broker thread implements the producer-side of the broker where the messages are sent and are later consumed by the web app.\nfor channel in channels: os.system(\u0026#34;iwconfig \u0026#34; + monitor_iface + \u0026#34; channel \u0026#34; + str(channel) + \u0026#34; \u0026gt; /dev/null 2\u0026gt;\u0026amp;1\u0026#34;)\tThe second thread, run_broker, is responsible for sending the sniffing data to the broker\ndef main(): #Chopper Thread chopper = threading.Thread(target=chopping) chopper.daemon = True chopper.start() #Broker Thread brokerthread = threading.Thread(target=run_broker) brokerthread.daemon = True brokerthread.start() #(...) Packet Handling    As probe request packets are being captured by PyShark, they are processed by a function, packetHandler(), which filters packets that only are sent by client devices searching for access points in broadcast. Therefore, all AP\u0026rsquo;s sending beacon frames advertising their SSID as well as devices already connected to a certain SSID/access point are all dropped. After this process, the function retrieves the mac address from the packet and records the current time. This pair of values are saved this in a dictionary and represent the current list of in range devices. Afterward, the same mac address is introduced into the unique devices set, in order to prevent repetitions of the same mac address. In the end of every iteration of this function, another function is called: peopleUpdate() which goes through the dictionary of the current devices and checks the difference between the current time and the time when each mac address was captured and if the difference is greater than 5 seconds that pair of value stored in the dictionary is removed.\nBroker    The broker connects to the IP address and a port of the broker installed on the Jetson\u0026rsquo;s post and publishes messages on 2 different topics: \u0026ldquo;sniffing/current\u0026rdquo; and \u0026ldquo;sniffing/unique\u0026rdquo;. The first topic is where the current number of devices is published every 5 seconds while the second topic contains the number of unique devices, sent every 60 seconds.\n"},{"id":7,"href":"/documentation/Sensors/","title":"Sensors","parent":"Documentation","content":"Jetson Nano    The object detection script runs on a Jetson Nano located at IT. Jetson Nano is a small and powerful computer for embedded applications that allows to run multiple neural networks. Due to its capacities, this device supports our object detection module.\n Local broker    During the detection in the video stream the number of detected objects is sent to a local broker in 6 different topics, which then sends these values to the central broker in IT.\nbroker_address = \u0026#34;####.nap.av.it.pt\u0026#34; port = 1883 #current currentppl_topic = \u0026#34;detection/people/current\u0026#34; currentvehicles_topic = \u0026#34;detection/vehicle/current\u0026#34; currenttwowheelers_topic = \u0026#34;detection/twowheelers/current\u0026#34; #unique uniqueppl_topic = \u0026#34;detection/people/unique\u0026#34; uniquevehicles_topic = \u0026#34;detection/vehicle/unique\u0026#34; uniquetwowheelers_topic = \u0026#34;detection/twowheelers/unique\u0026#34; APU    The sniffing script runs on two different APUs. One is located in a smart lamp post near the IT building and another is located in a smart lamp post in Cais da Fonte Nova.\n Local broker    During the detection of MAC address the number of devices is sent to a local broker in 2 different topics, which then sends these values to the central broker in IT.\nbroker_address = \u0026#34;####.nap.av.it.pt\u0026#34; port = 1883 currentppl_topic = \u0026#34;sniffing/current\u0026#34; uniqueppl_topic = \u0026#34;sniffing/unique\u0026#34;   -- Services    The detection running on the devices are started after the Linux kernel is booted using services units and systemd to manage them. The units are located in the folder /etc/systemd/system.\nAPUs    [Unit] Description=Wifi Sniffing Service After=multi-user.target [Service] WorkingDirectory=/root/WiFi_Sniffing User=root Type=idle ExecStart=/root/WiFi_Sniffing/venv/bin/python3 /root/WiFi_Sniffing/pyshark_ps.py wlan1 Restart=always [Install] WantedBy=multi-user.target Jetson Nano    [Unit] Description=People Detection Service [Service] User=root Type=forking #WorkingDirectory=/usr/local/bin ExecStart=/bin/bash /usr/local/bin/runDetection.sh Restart=always [Install] WantedBy=multi-user.target References    Jetson Nano information\nAPU information\n"},{"id":8,"href":"/documentation/Web-Application/REST-API/","title":"Rest API","parent":"Web Application","content":"Introduction    The goal of this API is to promote an easy access to all of the data that can be displayed in the dashboard. The data and our resources are accessed by URIs. The clients send requests to these URIs using the methods defined by the HTTP protocol to obtain data relative to that specific endpoint.\n   Action Method Description     Create POST Create a new unique object   Read GET Obtain information about a object or collection of objects    Endpoints    Sniffing Information    GET /api/sniffing\n  URL parameters start = [integer] end = [integer] bucket = day|hour|minute group = count|avg|sum|min|max\n  Example call https://dev.aveiro-open-lab.pt/api/sniffing?start=1621300462000\u0026amp;end=1621372462000\u0026amp;bucket=hour\u0026amp;group=count\n  GET /api/sniffingunique\n  URL parameters start = [integer] end = [integer] bucket = day|hour|minute group = count|avg|sum|min|max\n  Example call https://dev.aveiro-open-lab.pt/api/sniffingunique?start=1621300462000\u0026amp;end=1621372462000\u0026amp;bucket=hour\u0026amp;group=count\n  Detection Information    GET /api/people\n  URL parameters start = [integer] end = [integer] bucket = day|hour|minute group = count|avg|sum|min|max\n  Example call https://dev.aveiro-open-lab.pt/api/people?start=1621300462000\u0026amp;end=1621372462000\u0026amp;bucket=hour\u0026amp;group=count\n  GET /api/peopleunique\n  URL parameters start = [integer] end = [integer] bucket = day|hour|minute group = count|avg|sum|min|max\n  Example call https://dev.aveiro-open-lab.pt/api/peopleunique?start=1621300462000\u0026amp;end=1621372462000\u0026amp;bucket=hour\u0026amp;group=count\n  GET /api/vehicle\n  URL parameters start = [integer] end = [integer] bucket = day|hour|minute group = count|avg|sum|min|max\n  Example call https://dev.aveiro-open-lab.pt/api/vehicle?start=1621300462000\u0026amp;end=1621372462000\u0026amp;bucket=hour\u0026amp;group=count\n  GET /api/vehicleunique\n  URL parameters start = [integer] end = [integer] bucket = day|hour|minute group = count|avg|sum|min|max\n  Example call https://dev.aveiro-open-lab.pt/api/vehicleunique?start=1621300462000\u0026amp;end=1621372462000\u0026amp;bucket=hour\u0026amp;group=count\n  GET /api/twowheeler\n  URL parameters start = [integer] end = [integer] bucket = day|hour|minute group = count|avg|sum|min|max\n  Example call https://dev.aveiro-open-lab.pt/api/twowheeler?start=1621300462000\u0026amp;end=1621372462000\u0026amp;bucket=hour\u0026amp;group=count\n  GET /api/twowheelerunique\n  URL parameters start = [integer] end = [integer] bucket = day|hour|minute group = count|avg|sum|min|max\n  Example call https://dev.aveiro-open-lab.pt/api/twowheelerunique?start=1621300462000\u0026amp;end=1621372462000\u0026amp;bucket=hour\u0026amp;group=count\n  Sensors    These ends points return information related to the status of the services running on the different detection modules. This allows the application to know when the services stop running and to alert the administrator in case of such event.\nGET /api/sensors/p22_apu\n Example call https://dev.aveiro-open-lab.pt/api/sensors/p22_apu  GET /api/sensors/p1_apu\n Example call https://dev.aveiro-open-lab.pt/api/sensors/p1_apu  GET /api/sensors/pei_jetson\n Example call https://dev.aveiro-open-lab.pt/api/sensors/pei_jetson  Camera controls    There are also endpoints that control the movement of a rotational camera. All of these endpoints follow the the call structure https://dev.aveiro-open-lab.pt/api/cam/\u0026lt;action\u0026gt;.\nPOST | GET /api/cam/up POST | GET /api/cam/down POST | GET /api/cam/left POST | GET /api/cam/right\nPOST | GET /api/cam/upleft POST | GET /api/cam/upright POST | GET /api/cam/downright POST | GET /api/cam/downleft\nPOST | GET /api/cam/zoomin POST | GET /api/cam/zoomout\nPOST | GET /api/cam/focusin POST | GET /api/cam/focusout\nPOST | GET /api/cam/irisin POST | GET /api/cam/irisout\nPOST | GET /api/cam/auto POST | GET /api/cam/autostop\nAuthentication    There are reserved endpoints that allow the authentication of users and thus control the access to certain resources.\nPOST /login\n Example call https://dev.aveiro-open-lab.pt/login  POST /signup\n Example call https://dev.aveiro-open-lab.pt/signup  POST /logout\n Example call https://dev.aveiro-open-lab.pt/logout  "},{"id":9,"href":"/documentation/Web-Application/Structure/","title":"Structure","parent":"Web Application","content":"Basic architecture of the web application    The web application is a flask app, running on the gunicorn server and uses NGINX has a proxy server.\nFile Structure     __init__.py - initialization of the app main.py - main app running on server auth.py - management of the accounts: validation and intermediate on the access to the database /templates - folder containing all the Static rendered Jinja2 HTML files  /templates/template.html - contains the base HTML page for the dashboard /templates/detection.html - extends template and has the HTML code to show the detection of vehicles, devices, people and two-wheelers /templates/moliceiros.html - extends template and has the HTML code to show the detection data of moliceiros /templates/base.html - contains the base HTML page for managing the accounts /templates/login.html - extends base and has the HTML code to login into the account /templates/signup.html - extends base and has the HTML code to create an account    Main functions    The main functions of the app are:\n main.detection() - renders the template of the main page where the detection data of vehicles, people, devices and two-wheelers is showed. main.people() - renders the template of the page where the detection data of moliceiros is showed. auth.login() - takes care of the account authentication and the page is rendered based on the success of it auth.signup() - takes care of the creation of accounts and the page is rendered based on the success of it auth.logout() - logouts the account and redirects to the main page, the detection dashboard  To render the HTML pages with jinja the functions return render_template(\u0026lsquo;person.html\u0026rsquo;, pagename=\u0026ldquo;Moliceiros and People detection\u0026rdquo;) the argument pagename renders the page name on the html files\nBroker access    In flask, we access the broker and subscribe to topics to get real-time data. On these lines of code we access the broker\napp.config[\u0026#39;MQTT_BROKER_URL\u0026#39;] = \u0026#39;###.nap.av.it.pt\u0026#39; app.config[\u0026#39;MQTT_BROKER_PORT\u0026#39;] = 1883 app.config[\u0026#39;MQTT_REFRESH_TIME\u0026#39;] = 1.0 mqtt = Mqtt(app) mqtt = Mqtt(app) starts the broker access for the Broker. handle_connect subscribes to the topic, and handle_mqtt_message, gets the message in real-time and sends it through a socket to the web app\n@mqtt.on_connect() def handle_connect(client, userdata, flags, rc): mqtt.subscribe(\u0026#39;sniffing\u0026#39;) #where the topic is called \u0026#39;sniffing\u0026#39; @mqtt.on_message() def handle_mqtt_message(client, userdata, message): data = dict( topic=message.topic, payload=message.payload.decode() ) # emit a mqtt_message event to the socket containing the message data socketio.emit(\u0026#39;mqtt_message\u0026#39;, data=data) print(data) Server Sent Events (SSE)    To show in real time the broker data from flask to HTML, we need to open a socket using server sent events, for that in the app.py file app routes were created for the server sent events, where is returned a Response of type \u0026ldquo;text/event-stream\u0026rdquo;\nFor the html - template page, on the DIVS regarding the cards and charts a connection to the respective SSE is made, and the values are showed in real-time.\nAccount\u0026rsquo;s Database    In order to store and manage the user\u0026rsquo;s accounts it was created an SQLite database called user.\nFlask makes it more intuitive and practical with the integration of classes/libraries like: SQLAlchemy, LoginManager and UserMixin.\n SQLAlchemy - library that facilitates the communication between the app and the database LoginManager - class used to hold the settings used for logging in UserMixin - class that provides default implementations for methods that flask_login expects user objects to have  The configuration is done in the __init__.py file:\nfrom flask_sqlalchemy import SQLAlchemy from flask_login import LoginManager, UserMixin # ... # init SQLAlchemy db = SQLAlchemy() #define database class User(UserMixin, db.Model): id = db.Column(db.Integer, primary_key=True) # primary keys are required by SQLAlchemy email = db.Column(db.String(100), unique=True) password = db.Column(db.String(100)) name = db.Column(db.String(1000)) def create_app(): # ... app.config[\u0026#39;SQLALCHEMY_DATABASE_URI\u0026#39;] = \u0026#39;sqlite:///db.sqlite\u0026#39; #the path to the SQLite database file  app.config[\u0026#39;SQLALCHEMY_TRACK_MODIFICATIONS\u0026#39;] = False # deactivate Flask-SQLAlchemy track modifications db.init_app(app) # Initialiaze sqlite database # The login manager contains the code that lets your application and Flask-Login work together login_manager = LoginManager() # Create a Login Manager instance login_manager.login_view = \u0026#39;auth.login\u0026#39; # define the redirection path when login required and we attempt to access without being logged in login_manager.init_app(app) # configure it for login # ... Then the database is created in the __main__ function in the main.py file:\nif __name__ == \u0026#34;__main__\u0026#34;: # ... db.create_all(app=create_app()) # create the SQLite database Babel Configuration    Flask-Babel is an extension to Flask that helps get text translations with the help of Jinja.\nOn the createapp() function in the __init__.py file:\n#Babel Configuration app.config[\u0026#39;BABEL_DEFAULT_LOCALE\u0026#39;] = \u0026#39;en\u0026#39; #Creation of mqtt, socket and babel objects for the app babel = Babel(app) # set up babel @babel.localeselector def get_locale(): if request.args.get(\u0026#39;lang\u0026#39;): session[\u0026#39;lang\u0026#39;] = request.args.get(\u0026#39;lang\u0026#39;) return session.get(\u0026#39;lang\u0026#39;, \u0026#39;en\u0026#39;) To generate the translation files the following steps need to be made:\nRun the pybabel command that comes with Babel to extract your strings:\npybabel extract -F babel.cfg -o messages.pot . Generate the language translation file (ex.: pt)\npybabel init -i messages.pot -d translations -l pt After translating the web application strings on the generated file (ex.: pt.po) compile\npybabel compile -d translations "},{"id":10,"href":"/documentation/categories/","title":"Categories","parent":"Documentation","content":""},{"id":11,"href":"/documentation/diagrams/","title":"Diagrams","parent":"Documentation","content":""},{"id":12,"href":"/documentation/","title":"Documentation","parent":"","content":""},{"id":13,"href":"/documentation/modules/","title":"Modules","parent":"Documentation","content":""},{"id":14,"href":"/documentation/tags/","title":"Tags","parent":"Documentation","content":""},{"id":15,"href":"/documentation/web-application/","title":"Web Application","parent":"Documentation","content":""}]